\input{chpreamble}
  \chapter*{Introduction}
    \addcontentsline{toc}{chapter}{Introduction}
    \chaptermark{Introduction}
    \markboth{Introduction}{Introduction}

    For many students the study of mathematics consists mainly of the study of functions, a class of objects that reflect a common intuition -- a relationship between two quantities.
    Math textbooks present a plethora of example functions: the position of a car on the highway or a boat on the river, the balance of an interest-bearing bank account or even abstract mathematical relations like $y=x^2$.
    Such relations are the object of study in elementary algebra and calculus. 
    The prevalence of their study in mathematics indicates the utility of functional thinking and its applicability to widely varied problems.
    However, such concepts are limited in generality: if a problem fails to meet certain requirements we are forced to throw up our hands and abandon it, or to say that the answer is ``undefined'' or ``indeterminate.''
  %  \footnote{That many seemingly sensible problems lead to such results, at least in the context of elementary mathematics, is not only a frustration for students, but a failing of mathematics.}
    It is precisely such difficulties that encourage mathematicians to pursue ever-more-general concepts, with the hope of subsuming exceptions into a complete system, even if generalizations require abandoning some of the intuition behind an idea.

    Before continuing, it is worthwhile to clear up some terminology.
    The objects of study in this thesis are \emph{functions on the line}, meaning functions that take a single real number as input.
    The output of such a function is of less import, but will generally be another number, either real or complex.
    For the sake of simplicity and consistency (and noting that any real number is a complex number) I will use the term ``function'' to denote a function taking real numbers as inputs and giving complex numbers as outputs.
    For functions with different inputs (or outputs) I will use the term ``mapping'' to avoid confusion.
    Thus we say that a function has the set of real numbers as its \emph{domain} and the set of complex numbers as its \emph{codomain}, while a mapping may have a different domain or codomain.
    With this point squared away, we can return to issues of generality.

    In the case of \emph{functions}, an endlessly troublesome issue is differentiation---finding the rate of change of a function's output as the input varies.
    Unless a function is continuous (having no abrupt changes) we cannot differentiate it, and even then we have no guarantee that its derivative will itself be continuous.
    These issues can be avoided by allowing derivatives to be undefined at certain points, but this merely replaces one problem with another: that a derivative is no longer defined on the same domain as the function from which it originates.
    The field of measure theory and Lebesgue integration is one way of avoiding such issues, by allowing functions to misbehave, but only by a small amount.

    The theory of distributions makes an even larger step away from elementary function theory, by requiring only that distributions act \emph{like} functions in certain key ways.
    This means that any function is a valid distribution, but not all distributions are functions.
    The key concept from which we can build the theory of these distributions is that of a weighted average -- instead of evaluating a function at a specific point we can evaluate it over a range of inputs and combine the results to get an average answer.
    This is exactly what happens when we make most real-world measurements.
    For example, no thermometer can measure the temperature at exactly one point but instead will measure the average temperature of a small area.
    However, this is complicated by the fact that a thermometer may not be uniformly sensitive over that whole area -- a meat thermometer's sensitivity might vary along its length, meaning that its measurements are \emph{weighted} averages. 
    Mathematically, we can model the temperature of our cut of meat as a function that depends on the distance from the surface, say $f(x)$.
    Our thermometer's sensitivity can also be modeled as a function $\varphi(x)$ that depends on the same distance.
    Measuring the temperature of the meat with our thermometer can be described by an integral of $f$, weighted by $\varphi$: $T_\text{measured} = \int_0^\ell f(x)\varphi(x) dx$, where $\ell$ is the length of the thermometer.
    However, given that a thermometer is not sensitive to temperatures far away from itself, we can write $T_\text{measured} = \int_{-\infty}^{\infty} f(x)\varphi(x)dx$ without changing the result.

    We call $\varphi$ a \emph{test function} because we use it to test the value of $f$.
    By using different test functions (different thermometers) we can get a good idea of how $f$ behaves without ever evaluating $f$ at a specific point.
    This is the key observation of distribution theory -- (almost) any given function will give us a result when measured a test function.
    The result of this measurement is a number that depends both on the function we're testing and the test function we choose.
    Although we have yet to define precisely what we mean by a ``test function,'' we can now explain what we mean by a ``distribution'':

    \begin{defn}
      A \emph{distribution} is a mapping that takes \emph{test functions} as inputs and gives numbers as outputs.
      For a distribution $u$ and a test function $\varphi$, we denote this number $\langle u, \varphi \rangle$ instead of $u(\varphi)$ to emphasize the distinction between functions of real variables and distributions.

      A function $f$ gives rise to a distribution $u_f$ by integration against $f$:
      \begin{align*}
        \langle u_f, \varphi \rangle = \int_{-\infty}^{\infty} f(x)\varphi(x)dx \text{.}
      \end{align*}

    \end{defn}

    This initial definition is far from precise but, as the following chapters will show, it leads to the development of a neat and comprehensive general theory of functions.
    However, defining distributions in this way risks obscuring its original intent: to generalize functions of real numbers.
    As such, it is worthwhile to keep in mind the (imprecise) idea that distributions are a sort of ``function'' of real numbers that we can ``integrate'' against test functions, even though many distributions are hard, if not impossible, to conceive of in this way.
    A simple example of such a distribution is \emph{Dirac delta}:
    \begin{gather*}
      \langle\delta,\varphi\rangle = \varphi(0)
      \qquad\text{for any }\varphi \text{.}
    \end{gather*}
    If $\delta$ were a classical function of any sort, we would have
    \begin{gather*}
      \delta(x)=0\text{ for }x\ne0\\
      \int_{x\in\R} \delta(x)\,dx = 1
    \end{gather*}
    but for any $z=\delta(0)\in\C$ the integral must be zero.
    Thus $\delta$ is not a classical function, and so is a ``true distribution'' in that it does not arise from a function with pointwise values.
    Instead we think of $\delta$ as an infinitesimally narrow spike with area 1, so that integrating it against a function $\varphi$ gives $\varphi(0)$.
  
    The following three chapters develop the theory of a particular space of test functions and the corresponding space of distributions.
    \Cref{ch:fourier} develops a space of functions that is well behaved with regard to useful operations such as differentiation and Fourier transform.
    \Cref{ch:topons} applies the theory of topological spaces to this space of functions, culminating in the construction of a complete topological vector space $\S(\R)$ of test functions.
    \Cref{ch:dists} finally develops the topological space of distributions $\SS(\R)$ as the dual space of $\S(\R)$. 

\end{document}
