\chapter*{Introduction}
  \addcontentsline{toc}{chapter}{Introduction}
  \chaptermark{Introduction}
  \markboth{Introduction}{Introduction}

  For many students the study of mathematics consists mainly of the study of functions, a class of objects that reflect a common intuition -- a relationship between two quantities.
  Math textbooks present a plethora of example functions: the position of a car on the highway or a boat on the river, the balance of an interest-bearing bank account or even abstract mathematical relations like $y=x^2$.
  Such relations are the object of study in elementary algebra and calculus. 
  The prevalence of their study in mathematics indicates the utility of functional thinking and its applicability to widely varied problems.
  However, such concepts are limited in generality -- if a problem fails to meet certain requirements we are force to throw up our hands and abandon it, or to say that the answer is ``undefined'' or ``indeterminate''.
%  \footnote{That many seemingly sensible problems lead to such results, at least in the context of elementary mathematics, is not only a frustration for students, but a failing of mathematics.}
  It is precisely such difficulties that encourage mathematicians to pursue ever-more-general concepts, with the hope of subsuming exceptions into a complete system, even if generalisations require abandoning some of the intuition behind an idea.

  In the case of functions, an endlessly troublesome issue is differentiation -- finding the rate of change of a function's output as the input varies.
  Unless a function is continuous (having no abrupt changes) we cannot differentiate it, and even then we have no guarantee that its derivative will itself be continuous.
  These issues can be avoided by allowing derivatives to be undefined at certain points, but this merely replaces one problem with another -- namely that a derivative is no longer defined on the same domain as the function from which it originates.
  The field of measure theory and Lebesgue integration is one way of avoiding such issues, by allowing functions to misbehave, but only by a small amount.

  The theory of distributions makes an even larger step away from elementary function theory, by requiring only that distributions act \emph{like} functions in certain key ways.
  This means that any function is a valid distribution, but not all distributions are functions.
  The key concept from which we can build the theory of these distributions is that of a weighted average -- instead of evaluating a function at a specific point we can evaluate it over a range of inputs and combine the results to get an average answer.
  This is exactly what happens when we make most real-world measurements.
  For example, no thermometer can measure the temperature at exactly one point -- instead it will measure the average temperature of a small area.
  However, this is complicated by the fact that a thermometer may not be uniformly sensitive over that whole area -- a meat thermometer's sensitivity might vary along its length, meaning that its measurements are \emph{weighted} averages. 
  Mathematically, we can model the temperature of our cut of meat as a function that depends on the distance from the surface, say $f(x)$.
  Our thermometer's sensitivity can also be modeled as a function $\varphi(x)$ that depends on the same distance.
  Measuring the temperature of the meat with our thermometer can be described with an integral of $f$, weighted by $\varphi$: $T_\text{meat} = \int f(x)\varphi(x) dx$ %TODO:endpoints

  We call the functions $\varphi$ a \emph{test function} because we use it to test the value of $f$.
  By using different test functions (different thermometers) we can get a good idea of how $f$ behaves without ever evaluating $f$ at a specific point.
  This is the key observation of distribution theory -- any given function will give us a result when probed with a test function.
